{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_path = \"C:/Users/arwin/Documents/dev/APS360-PROJECT/data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Genre Classification Model Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigModel(nn.Module):\n",
    "  def  __init__(self):\n",
    "    super(BigModel, self).__init__()\n",
    "    # CNN\n",
    "    resnet18 = models.resnet18(pretrained=True)\n",
    "    self.features = nn.Sequential(*(list(resnet18.children())[:-2])) # I removed the last two layers of resnet and replaced them with the ones said in the paper\n",
    "    self.features.add_module(\"pool\", nn.AdaptiveMaxPool2d((1,1)))\n",
    "\n",
    "    self.fc1_cnn = nn.Linear(512, 256)\n",
    "    self.dropout_cnn = nn.Dropout(p = 0.5)\n",
    "    self.batch_norm_cnn = nn.BatchNorm1d(256)\n",
    "    \n",
    "    # RNN\n",
    "    self.bi_gru = nn.GRU(input_size=672, hidden_size = 256, num_layers =1, batch_first=True, bidirectional=True) # I dont know what the size of the input to the bi-gru is supposed to be\n",
    "    self.fc_gru = nn.Linear(512, 256)\n",
    "    \n",
    "    # FC LAYERS\n",
    "    self.dropout = nn.Dropout(p = 0.5)\n",
    "    # self.fc = nn.Linear(512, 1)\n",
    "    self.fc = nn.Linear(512, 10) # becuase 10 genre classes\n",
    "    # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # CNN\n",
    "    # print(f\"Shape before ResNet features: {x.shape}\")  # Debug statement 1\n",
    "    x1 = self.features(x)\n",
    "    # print(f\"Shape after ResNet features: {x1.shape}\")  # Debug statement 1\n",
    "    x1 = torch.flatten(x1, 1)\n",
    "    # print(f\"Shape after flattening: {x1.shape}\")  # Debug statement 2\n",
    "    x1 = self.fc1_cnn(x1)\n",
    "    x1 = self.dropout_cnn(x1)\n",
    "    x1 = self.batch_norm_cnn(x1)\n",
    "    \n",
    "    # Bi-GRU\n",
    "    batch_size, channels, height, width = x.shape\n",
    "    x2 = x.view(batch_size, height, -1)\n",
    "    # print(f\"Shape before Bi-GRU: {x2.shape}\")  # Debug statement 4\n",
    "    x2, _ = self.bi_gru(x2)\n",
    "    x2 = torch.cat((x2[:, -1, :256], x2[:, 0, 256:]), dim=1)\n",
    "    x2 = self.fc_gru(x2)\n",
    "\n",
    "    # Concat Outputs of each model\n",
    "    x = torch.cat((x1, x2), -1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc(x)\n",
    "    # x = self.sigmoid(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(batch_size, learning_rate, epoch, weight_decay):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"transfermodel_bs{0}_lr{1}_epoch{2}_wd{3}\".format(\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch,\n",
    "                                                   weight_decay)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Modify The Pretrained Model by freezing CNN, Bi-Gru Layers and modifying fc layers for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_model_for_binary_classification(model):\n",
    "    # Freeze feature layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.bi_gru.parameters():\n",
    "        param.requires_grad = False\n",
    "    # for param in model.fc1_cnn.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    # for param in model.fc_gru.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    # for param in model.batch_norm_cnn.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # Remove and add new FC layers for binary classification\n",
    "    # Adjust the input features of the first linear layer according to your new architecture\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(512, 256),  # Adjust the input size if needed\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(256, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Dataset Class that has 2 classes or Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dirs, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dirs (dict): Dictionary with keys 'hit' and 'miss' and their respective image directory paths.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.img_labels = []\n",
    "        self.img_files = []\n",
    "        for label, dir_path in img_dirs.items():\n",
    "            for file in os.listdir(dir_path):\n",
    "                if file.endswith('.png'):\n",
    "                    self.img_files.append(os.path.join(dir_path, file))\n",
    "                    self.img_labels.append(1 if label=='hit' else 0)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.img_labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label, dtype=torch.float) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img_dirs = {\n",
    "    'hit': data_path + '/billboard_spec',\n",
    "    'miss': data_path + '/random_spec'\n",
    "}\n",
    "\n",
    "# Initialize your dataset\n",
    "dataset = CustomImageDataset(img_dirs=img_dirs, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size + test_size])\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(val_test_dataset, [val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, batch_size, learning_rate, num_epochs, weight_decay, save=True):\n",
    "    \n",
    "    # data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Setup train\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Lists to keep track of progress\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "\n",
    "    # Accuracy calculation function\n",
    "    def binary_accuracy(preds, y):\n",
    "        rounded_preds = torch.round(preds)\n",
    "        correct = (rounded_preds == y).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = num_epochs\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Training\n",
    "        iteration = 1\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)  # Adjust for label dimensions\n",
    "\n",
    "            # print(\"input shape: \", inputs.shape, \" labels shape: \", labels.shape)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            # print(\"outputs shape: \", outputs.shape)\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = binary_accuracy(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += acc.item() * inputs.size(0)\n",
    "\n",
    "            # print(f\"Iteration {iteration} complete.\")\n",
    "            iteration += 1 \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "\n",
    "        # print(\"validating...\")\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_running_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)  # Adjust for label dimensions\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                acc = binary_accuracy(outputs, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "                val_running_corrects += acc.item() * inputs.size(0)\n",
    "        \n",
    "        val_loss = val_running_loss / len(val_loader.dataset)\n",
    "        val_acc = val_running_corrects / len(val_loader.dataset)\n",
    "\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "    \n",
    "    # Optional: Save model and training history\n",
    "    if save:\n",
    "        model_name = get_model_name(batch_size=batch_size, learning_rate=learning_rate, epoch=num_epochs, weight_decay=weight_decay)\n",
    "        dir = f'{model_name}/'\n",
    "        if not os.path.exists(dir):\n",
    "            os.makedirs(dir)\n",
    "        torch.save(model.state_dict(), f'{model_name}/{model_name}.pth')\n",
    "        np.savez(f'{model_name}/{model_name}.npz', train_losses=train_losses, train_accuracies=train_accuracies, val_losses=val_losses, val_accuracies=val_accuracies)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(model_name):\n",
    "    # Load the saved training history\n",
    "\n",
    "    data = np.load(f'{model_name}/{model_name}.npz')\n",
    "\n",
    "    # Extracting the training and validation losses and accuracies\n",
    "    train_losses = data['train_losses']\n",
    "    val_losses = data['val_losses']\n",
    "    train_accuracies = data['train_accuracies']\n",
    "    val_accuracies = data['val_accuracies']\n",
    "\n",
    "    # Plotting\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "    # Plot training and validation losses\n",
    "\n",
    "    # Example for setting text size individually\n",
    "    axs[0].set_xlabel('Epochs', fontsize=20)  # Adjust fontsize as needed\n",
    "    axs[0].set_ylabel('Loss', fontsize=20)  # Adjust fontsize as needed\n",
    "    axs[0].set_title(f'Training and Validation Loss for {model_name}', fontsize=16)  # Adjust fontsize as needed\n",
    "    axs[0].legend(fontsize=20)\n",
    "    axs[0].set_xlim(0, len(train_losses))  # Set x-axis range from 0 to number of epochs\n",
    "    # axs[0].set_ylim(0, max(max(train_losses), max(val_losses)))  # Set y-axis range\n",
    "\n",
    "    # And similarly for other text elements\n",
    "\n",
    "    axs[0].plot(train_losses, label='Training Loss')\n",
    "    axs[0].plot(val_losses, label='Validation Loss')\n",
    "\n",
    "\n",
    "\n",
    "    # Plot training and validation accuracies\n",
    "    axs[1].plot(train_accuracies, label='Training Accuracy')\n",
    "    axs[1].plot(val_accuracies, label='Validation Accuracy')\n",
    "    axs[1].set_xlabel('Epochs', fontsize=20)\n",
    "    axs[1].set_ylabel('Accuracy', fontsize=20)\n",
    "    axs[1].set_title(f'Training and Validation Accuracy for {model_name}',fontsize=16)\n",
    "    axs[1].legend()\n",
    "    axs[1].set_xlim(0, len(train_accuracies))  # Set x-axis range from 0 to number of epochs\n",
    "    # axs[1].set_ylim(0, 1)  # Assuming accuracy ranges from 0 to 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Also Load pretrained weights and call modify function \\\n",
    "Set hyperparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arwin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\arwin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = BigModel()\n",
    "path_to_saved_model = 'C:/Users/arwin/Documents/dev/APS360-PROJECT/genre_classifier/genre_ensemble_model_batch256_lr0.001_weightdecay3e-05.pth' \n",
    "model.load_state_dict(torch.load(path_to_saved_model))\n",
    "model = modify_model_for_binary_classification(model)\n",
    "model = model.to(device)\n",
    "# summary(model, input_size=(3, 224, 224))\n",
    "\n",
    "batch_size = 64\n",
    "learning_rate = 0.0004\n",
    "num_epochs = 100\n",
    "weight_decay = 1e-4\n",
    "save = True\n",
    "\n",
    "train(model, batch_size=batch_size, learning_rate=learning_rate, num_epochs=num_epochs, weight_decay=weight_decay, save=save)\n",
    "model_name=get_model_name(batch_size=batch_size, learning_rate=learning_rate, epoch=num_epochs, weight_decay=weight_decay)\n",
    "plot_model(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
