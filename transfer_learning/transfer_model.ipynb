{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_path = \"C:/Users/arwin/Documents/dev/APS360-PROJECT/data\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Genre Classification Model Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigModel(nn.Module):\n",
    "  def  __init__(self):\n",
    "    super(BigModel, self).__init__()\n",
    "    # CNN\n",
    "    resnet18 = models.resnet18(pretrained=True)\n",
    "    self.features = nn.Sequential(*(list(resnet18.children())[:-2])) # I removed the last two layers of resnet and replaced them with the ones said in the paper\n",
    "    self.features.add_module(\"pool\", nn.AdaptiveMaxPool2d((1,1)))\n",
    "\n",
    "    self.fc1_cnn = nn.Linear(512, 256)\n",
    "    self.dropout_cnn = nn.Dropout(p = 0.5)\n",
    "    self.batch_norm_cnn = nn.BatchNorm1d(256)\n",
    "    \n",
    "    # RNN\n",
    "    self.bi_gru = nn.GRU(input_size=672, hidden_size = 256, num_layers =1, batch_first=True, bidirectional=True) # I dont know what the size of the input to the bi-gru is supposed to be\n",
    "    self.fc_gru = nn.Linear(512, 256)\n",
    "    \n",
    "    # FC LAYERS\n",
    "    self.dropout = nn.Dropout(p = 0.5)\n",
    "    # self.fc = nn.Linear(512, 1)\n",
    "    self.fc = nn.Linear(512, 10) # becuase 10 genre classes\n",
    "    # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # CNN\n",
    "    # print(f\"Shape before ResNet features: {x.shape}\")  # Debug statement 1\n",
    "    x1 = self.features(x)\n",
    "    # print(f\"Shape after ResNet features: {x1.shape}\")  # Debug statement 1\n",
    "    x1 = torch.flatten(x1, 1)\n",
    "    # print(f\"Shape after flattening: {x1.shape}\")  # Debug statement 2\n",
    "    x1 = self.fc1_cnn(x1)\n",
    "    x1 = self.dropout_cnn(x1)\n",
    "    x1 = self.batch_norm_cnn(x1)\n",
    "    \n",
    "    # Bi-GRU\n",
    "    batch_size, channels, height, width = x.shape\n",
    "    x2 = x.view(batch_size, height, -1)\n",
    "    # print(f\"Shape before Bi-GRU: {x2.shape}\")  # Debug statement 4\n",
    "    x2, _ = self.bi_gru(x2)\n",
    "    x2 = torch.cat((x2[:, -1, :256], x2[:, 0, 256:]), dim=1)\n",
    "    x2 = self.fc_gru(x2)\n",
    "\n",
    "    # Concat Outputs of each model\n",
    "    x = torch.cat((x1, x2), -1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc(x)\n",
    "    # x = self.sigmoid(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Modify The Pretrained Model by freezing CNN, Bi-Gru Layers and modifying fc layers for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_model_for_binary_classification(model):\n",
    "    # Freeze feature layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.bi_gru.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.fc1_cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.fc_gru.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.batch_norm_cnn.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Remove and add new FC layers for binary classification\n",
    "    # Adjust the input features of the first linear layer according to your new architecture\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(512, 256),  # Adjust the input size if needed\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(256, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pretrained weights and call modify function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arwin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\arwin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveMaxPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 256]         131,328\n",
      "          Dropout-69                  [-1, 256]               0\n",
      "      BatchNorm1d-70                  [-1, 256]             512\n",
      "              GRU-71  [[-1, 224, 512], [-1, 2, 256]]               0\n",
      "           Linear-72                  [-1, 256]         131,328\n",
      "          Dropout-73                  [-1, 512]               0\n",
      "           Linear-74                  [-1, 256]         131,328\n",
      "             ReLU-75                  [-1, 256]               0\n",
      "          Dropout-76                  [-1, 256]               0\n",
      "           Linear-77                    [-1, 1]             257\n",
      "          Sigmoid-78                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 11,571,265\n",
      "Trainable params: 131,585\n",
      "Non-trainable params: 11,439,680\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 385.20\n",
      "Params size (MB): 44.14\n",
      "Estimated Total Size (MB): 429.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = BigModel()\n",
    "path_to_saved_model = 'C:/Users/arwin/Documents/dev/APS360-PROJECT/genre_classifier/genre_ensemble_model_batch256_lr0.001_weightdecay3e-05.pth' \n",
    "model.load_state_dict(torch.load(path_to_saved_model))\n",
    "model = modify_model_for_binary_classification(model)\n",
    "model = model.to(device)\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Image Dataset Class that has 2 classes or Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dirs, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dirs (dict): Dictionary with keys 'hit' and 'miss' and their respective image directory paths.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.img_labels = []\n",
    "        self.img_files = []\n",
    "        for label, dir_path in img_dirs.items():\n",
    "            for file in os.listdir(dir_path):\n",
    "                if file.endswith('.png'):\n",
    "                    self.img_files.append(os.path.join(dir_path, file))\n",
    "                    self.img_labels.append(1 if label=='hit' else 0)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.img_labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label, dtype=torch.float) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img_dirs = {\n",
    "    'hit': data_path + '/billboard_spec',\n",
    "    'miss': data_path + '/random_spec'\n",
    "}\n",
    "\n",
    "# Initialize your dataset\n",
    "dataset = CustomImageDataset(img_dirs=img_dirs, transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size + test_size])\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(val_test_dataset, [val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.6100, Train Acc: 0.6733, Val Loss: 0.5895, Val Acc: 0.6759\n",
      "Epoch 2, Train Loss: 0.5983, Train Acc: 0.6775, Val Loss: 0.5781, Val Acc: 0.6932\n",
      "Epoch 3, Train Loss: 0.6026, Train Acc: 0.6733, Val Loss: 0.5796, Val Acc: 0.6967\n",
      "Epoch 4, Train Loss: 0.6103, Train Acc: 0.6807, Val Loss: 0.5739, Val Acc: 0.7174\n",
      "Epoch 5, Train Loss: 0.6006, Train Acc: 0.6735, Val Loss: 0.5778, Val Acc: 0.7116\n",
      "Epoch 6, Train Loss: 0.5948, Train Acc: 0.6819, Val Loss: 0.5938, Val Acc: 0.6794\n",
      "Epoch 7, Train Loss: 0.5983, Train Acc: 0.6836, Val Loss: 0.5778, Val Acc: 0.7174\n",
      "Epoch 8, Train Loss: 0.5975, Train Acc: 0.6762, Val Loss: 0.5747, Val Acc: 0.7047\n",
      "Epoch 9, Train Loss: 0.5903, Train Acc: 0.6896, Val Loss: 0.5761, Val Acc: 0.7163\n",
      "Epoch 10, Train Loss: 0.5946, Train Acc: 0.6873, Val Loss: 0.5899, Val Acc: 0.6932\n",
      "Epoch 11, Train Loss: 0.5931, Train Acc: 0.6819, Val Loss: 0.5658, Val Acc: 0.7151\n",
      "Epoch 12, Train Loss: 0.5993, Train Acc: 0.6804, Val Loss: 0.5681, Val Acc: 0.7093\n",
      "Epoch 13, Train Loss: 0.5909, Train Acc: 0.6873, Val Loss: 0.5790, Val Acc: 0.7116\n",
      "Epoch 14, Train Loss: 0.5924, Train Acc: 0.6834, Val Loss: 0.5757, Val Acc: 0.7036\n",
      "Epoch 15, Train Loss: 0.5932, Train Acc: 0.6841, Val Loss: 0.5710, Val Acc: 0.7105\n",
      "Epoch 16, Train Loss: 0.5918, Train Acc: 0.6863, Val Loss: 0.5668, Val Acc: 0.7082\n",
      "Epoch 17, Train Loss: 0.5909, Train Acc: 0.6851, Val Loss: 0.5706, Val Acc: 0.7001\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Lists to keep track of progress\n",
    "train_losses, val_losses = [], []\n",
    "train_accuracies, val_accuracies = [], []\n",
    "\n",
    "# Accuracy calculation function\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(preds)\n",
    "    correct = (rounded_preds == y).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# Training loop\n",
    "num_epochs = num_epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    \n",
    "    # Training\n",
    "    iteration = 1\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).float().unsqueeze(1)  # Adjust for label dimensions\n",
    "\n",
    "        # print(\"input shape: \", inputs.shape, \" labels shape: \", labels.shape)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        # print(\"outputs shape: \", outputs.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = binary_accuracy(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += acc.item() * inputs.size(0)\n",
    "\n",
    "        # print(f\"Iteration {iteration} complete.\")\n",
    "        iteration += 1 \n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_corrects / len(train_loader.dataset)\n",
    "\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracies.append(epoch_acc)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_running_loss = 0.0\n",
    "    val_running_corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).unsqueeze(1)  # Adjust for label dimensions\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = binary_accuracy(outputs, labels)\n",
    "\n",
    "            val_running_loss += loss.item() * inputs.size(0)\n",
    "            val_running_corrects += acc.item() * inputs.size(0)\n",
    "    \n",
    "    val_loss = val_running_loss / len(val_loader.dataset)\n",
    "    val_acc = val_running_corrects / len(val_loader.dataset)\n",
    "\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# Optional: Save model and training history\n",
    "torch.save(model.state_dict(), f'transfer_model_epochs{epoch+1}_batchsz{batch_size}_lr{learning_rate}.pth')\n",
    "np.savez('training_history1.npz', train_losses=train_losses, train_accuracies=train_accuracies, val_losses=val_losses, val_accuracies=val_accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Load the saved training history\n",
    "\n",
    "data = np.load('training_history1.npz')\n",
    "\n",
    "# Extracting the training and validation losses and accuracies\n",
    "train_losses = data['train_losses']\n",
    "val_losses = data['val_losses']\n",
    "train_accuracies = data['train_accuracies']\n",
    "val_accuracies = data['val_accuracies']\n",
    "\n",
    "# Plotting\n",
    "fig, axs = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "# Plot training and validation losses\n",
    "\n",
    "# Example for setting text size individually\n",
    "axs[0].set_xlabel('Epochs', fontsize=20)  # Adjust fontsize as needed\n",
    "axs[0].set_ylabel('Loss', fontsize=20)  # Adjust fontsize as needed\n",
    "axs[0].set_title('Training and Validation Loss', fontsize=20)  # Adjust fontsize as needed\n",
    "axs[0].legend(fontsize=20)\n",
    "\n",
    "# And similarly for other text elements\n",
    "\n",
    "axs[0].plot(train_losses, label='Training Loss')\n",
    "axs[0].plot(val_losses, label='Validation Loss')\n",
    "\n",
    "\n",
    "\n",
    "# Plot training and validation accuracies\n",
    "axs[1].plot(train_accuracies, label='Training Accuracy')\n",
    "axs[1].plot(val_accuracies, label='Validation Accuracy')\n",
    "axs[1].set_xlabel('Epochs', fontsize=20)\n",
    "axs[1].set_ylabel('Accuracy', fontsize=20)\n",
    "axs[1].set_title('Training and Validation Accuracy',fontsize=20)\n",
    "axs[1].legend()\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save model and training history\n",
    "torch.save(model.state_dict(), f'transfer_model_batchsz{batch_size}_lr{learning_rate}_epoch{num_epochs}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
