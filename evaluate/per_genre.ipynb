{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "import os\n",
    "from torchvision import transforms\n",
    "import csv\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "best_model = os.path.normpath(\"C:/Users/arwin/Documents/dev/APS360-PROJECT/transfer_learning/transfermodel_bs128_lr0.0005_epoch75_wd0/transfermodel_bs128_lr0.0005_epoch75_wd0.pth\")\n",
    "\n",
    "bb_path = os.path.normpath('C:/Users/arwin/Documents/dev/APS360-PROJECT/data/billboard_spec')\n",
    "rand_path = os.path.normpath('C:/Users/arwin/Documents/dev/APS360-PROJECT/data/random_spec')  \n",
    "\n",
    "bb_csv = os.path.normpath('C:/Users/arwin/Documents/dev/APS360-PROJECT/data/billboard_genres_filled.csv')  \n",
    "rand_csv = os.path.normpath('C:/Users/arwin/Documents/dev/APS360-PROJECT/data/random_songs_sampled_genres_filled.csv')  \n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Transfer Learning Model Class (and modification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigModel(nn.Module):\n",
    "  def  __init__(self):\n",
    "    super(BigModel, self).__init__()\n",
    "    # CNN\n",
    "    resnet18 = models.resnet18(pretrained=True)\n",
    "    self.features = nn.Sequential(*(list(resnet18.children())[:-2])) # I removed the last two layers of resnet and replaced them with the ones said in the paper\n",
    "    self.features.add_module(\"pool\", nn.AdaptiveMaxPool2d((1,1)))\n",
    "\n",
    "    self.fc1_cnn = nn.Linear(512, 256)\n",
    "    self.dropout_cnn = nn.Dropout(p = 0.5)\n",
    "    self.batch_norm_cnn = nn.BatchNorm1d(256)\n",
    "    \n",
    "    # RNN\n",
    "    self.bi_gru = nn.GRU(input_size=672, hidden_size = 256, num_layers =1, batch_first=True, bidirectional=True) # I dont know what the size of the input to the bi-gru is supposed to be\n",
    "    self.fc_gru = nn.Linear(512, 256)\n",
    "    \n",
    "    # FC LAYERS\n",
    "    self.dropout = nn.Dropout(p = 0.5)\n",
    "    # self.fc = nn.Linear(512, 1)\n",
    "    self.fc = nn.Linear(512, 10) # becuase 10 genre classes\n",
    "    # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # CNN\n",
    "    # print(f\"Shape before ResNet features: {x.shape}\")  # Debug statement 1\n",
    "    x1 = self.features(x)\n",
    "    # print(f\"Shape after ResNet features: {x1.shape}\")  # Debug statement 1\n",
    "    x1 = torch.flatten(x1, 1)\n",
    "    # print(f\"Shape after flattening: {x1.shape}\")  # Debug statement 2\n",
    "    x1 = self.fc1_cnn(x1)\n",
    "    x1 = self.dropout_cnn(x1)\n",
    "    x1 = self.batch_norm_cnn(x1)\n",
    "    \n",
    "    # Bi-GRU\n",
    "    batch_size, channels, height, width = x.shape\n",
    "    x2 = x.view(batch_size, height, -1)\n",
    "    # print(f\"Shape before Bi-GRU: {x2.shape}\")  # Debug statement 4\n",
    "    x2, _ = self.bi_gru(x2)\n",
    "    x2 = torch.cat((x2[:, -1, :256], x2[:, 0, 256:]), dim=1)\n",
    "    x2 = self.fc_gru(x2)\n",
    "\n",
    "    # Concat Outputs of each model\n",
    "    x = torch.cat((x1, x2), -1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc(x)\n",
    "    # x = self.sigmoid(x)\n",
    "    return x\n",
    "\n",
    "def modify_model_for_binary_classification(model):\n",
    "    # Freeze feature layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.bi_gru.parameters():\n",
    "        param.requires_grad = False\n",
    "    # for param in model.fc1_cnn.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    # for param in model.fc_gru.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    # for param in model.batch_norm_cnn.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # Remove and add new FC layers for binary classification\n",
    "    # Adjust the input features of the first linear layer according to your new architecture\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(512, 256),  # Adjust the input size if needed\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(256, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image(image_name, label):\n",
    "    if label == 'hit':\n",
    "        filepath = os.path.join(bb_path, image_name)\n",
    "    elif label == 'miss':\n",
    "        filepath = os.path.join(rand_path, image_name)\n",
    "    \n",
    "    # edge cases\n",
    "    else: return None\n",
    "    if not os.path.isfile(filepath): return None\n",
    "    \n",
    "    image = transforms.ToTensor()(Image.open(filepath).convert('RGB'))\n",
    "    \n",
    "    return image, torch.tensor(label=='hit', dtype=torch.float)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and Import Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arwin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\arwin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveMaxPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 256]         131,328\n",
      "          Dropout-69                  [-1, 256]               0\n",
      "      BatchNorm1d-70                  [-1, 256]             512\n",
      "              GRU-71  [[-1, 224, 512], [-1, 2, 256]]               0\n",
      "           Linear-72                  [-1, 256]         131,328\n",
      "          Dropout-73                  [-1, 512]               0\n",
      "           Linear-74                  [-1, 256]         131,328\n",
      "             ReLU-75                  [-1, 256]               0\n",
      "          Dropout-76                  [-1, 256]               0\n",
      "           Linear-77                    [-1, 1]             257\n",
      "          Sigmoid-78                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 11,571,265\n",
      "Trainable params: 394,753\n",
      "Non-trainable params: 11,176,512\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 385.20\n",
      "Params size (MB): 44.14\n",
      "Estimated Total Size (MB): 429.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = BigModel()\n",
    "model = modify_model_for_binary_classification(model)\n",
    "model.load_state_dict(torch.load(best_model))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arwin\\AppData\\Local\\Temp\\ipykernel_17180\\3089249684.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(1)\n",
      "C:\\Users\\arwin\\AppData\\Local\\Temp\\ipykernel_17180\\3089249684.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  label = torch.tensor(label, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(1)\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "rand_df = pd.read_csv(rand_csv)\n",
    "bb_df = pd.read_csv(bb_csv)\n",
    "\n",
    "# valid genres\n",
    "genres = ['pop', 'hiphop', 'rock', 'country', 'jazz', 'disco', 'reggae', 'metal', 'classical', 'blues']\n",
    "\n",
    "bb_song_preds = {genre: [] for genre in genres}\n",
    "bb_song_labels = {genre: [] for genre in genres}\n",
    "\n",
    "\n",
    "for index, row in bb_df.iterrows():\n",
    "    if row['Genre'] not in genres:\n",
    "        continue\n",
    "\n",
    "    input_label = prepare_image(f\"{row['Artist']}_{row['Song']}.png\", 'hit')\n",
    "    if input_label is None:\n",
    "        continue\n",
    "    \n",
    "    input, label = input_label\n",
    "    input = input.to(device)\n",
    "    label = torch.tensor(label, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(1)  \n",
    "\n",
    "    output = model(input.unsqueeze(0))  # batch dimension\n",
    "\n",
    "    bb_song_preds[row['Genre']].append(output.detach().cpu().numpy()) \n",
    "    bb_song_labels[row['Genre']].append(label.detach().cpu().numpy())\n",
    "\n",
    "# label size should match pred size across all genres\n",
    "assert(sum(len(lst) for lst in bb_song_preds.values()) ==  sum(len(lst) for lst in bb_song_labels.values()))\n",
    "# only pred same number of rand songs as bb songs\n",
    "total_bb_preds =  sum(len(lst) for lst in bb_song_preds.values())\n",
    "\n",
    "rand_song_preds = {genre: [] for genre in genres}\n",
    "rand_song_labels = {genre: [] for genre in genres}\n",
    "\n",
    "\n",
    "for index, row in rand_df.iterrows():\n",
    "    if row['Genre'] not in genres:\n",
    "        continue\n",
    "\n",
    "    input_label = prepare_image(f\"{row['Artist']}_{row['Song']}.png\", 'miss')\n",
    "    if input_label is None:\n",
    "        continue\n",
    "    \n",
    "    input, label = input_label\n",
    "    input = input.to(device)\n",
    "    label = torch.tensor(label, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(1)  \n",
    "\n",
    "    output = model(input.unsqueeze(0))  # batch dimension\n",
    "\n",
    "    rand_song_preds[row['Genre']].append(output.detach().cpu().numpy()) \n",
    "    rand_song_labels[row['Genre']].append(label.detach().cpu().numpy())\n",
    "\n",
    "    if sum(len(lst) for lst in rand_song_preds.values()) == total_bb_preds:\n",
    "        break\n",
    "\n",
    "# now do final assertations\n",
    "\n",
    "total_rand_preds = sum(len(lst) for lst in rand_song_preds.values())\n",
    "total_rand_labels = sum(len(lst) for lst in rand_song_labels.values())\n",
    "assert(total_rand_preds == total_rand_labels)\n",
    "assert(total_rand_preds == total_bb_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 565)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_bb_preds, total_rand_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hits\n",
    "bb_genre_accs = {genre: 0 for genre in genres}\n",
    "for genre, pred_list in bb_song_preds.items():\n",
    "\n",
    "    genre_preds_binary = [1 if pred >= 0.5 else 0 for pred in pred_list]\n",
    "    correct = sum([1 for pred, label in zip(genre_preds_binary, bb_song_labels[genre]) if pred == label])\n",
    "\n",
    "    if len(bb_song_labels[genre]) == 0:\n",
    "        bb_genre_accs[genre] = 0\n",
    "    else:\n",
    "        bb_genre_accs[genre] = correct / len(bb_song_labels[genre])\n",
    "\n",
    "# misses\n",
    "rand_genre_accs = {genre: 0 for genre in genres}\n",
    "for genre, pred_list in rand_song_preds.items():\n",
    "\n",
    "    genre_preds_binary = [1 if pred >= 0.5 else 0 for pred in pred_list]\n",
    "    correct = sum([1 for pred, label in zip(genre_preds_binary, rand_song_labels[genre]) if pred == label])\n",
    "\n",
    "    if len(rand_song_labels[genre]) == 0:\n",
    "        rand_genre_accs[genre] = 0\n",
    "    else:\n",
    "        rand_genre_accs[genre] = correct / len(rand_song_labels[genre])\n",
    "\n",
    "# Combined accuracies\n",
    "combined_genre_accs = {genre: 0 for genre in genres}\n",
    "\n",
    "for genre in genres:\n",
    "    total_preds = len(bb_song_preds[genre]) + len(rand_song_preds[genre])\n",
    "    \n",
    "    bb_correct_preds = sum([1 if pred >= 0.5 else 0 for pred, label in zip(np.squeeze(bb_song_preds[genre]), np.squeeze(bb_song_labels[genre])) if (pred >= 0.5) == bool(label)])\n",
    "    rand_correct_preds = sum([1 if pred >= 0.5 else 0 for pred, label in zip(np.squeeze(rand_song_preds[genre]), np.squeeze(rand_song_labels[genre])) if (pred >= 0.5) == bool(label)])\n",
    "    total_correct_preds = bb_correct_preds + rand_correct_preds\n",
    "\n",
    "    if total_preds > 0:\n",
    "        combined_genre_accs[genre] = total_correct_preds / total_preds\n",
    "    else:\n",
    "        combined_genre_accs[genre] = 0  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMBINED\n",
      "pop 0.6153846153846154\n",
      "hiphop 0.48741007194244607\n",
      "rock 0.09433962264150944\n",
      "country 0.5955882352941176\n",
      "jazz 0.0\n",
      "disco 0.0\n",
      "reggae 0.42857142857142855\n",
      "metal 0.0\n",
      "classical 0.0\n",
      "blues 0.0\n",
      "HITS\n",
      "pop 0.8823529411764706\n",
      "hiphop 0.8685897435897436\n",
      "rock 0.7692307692307693\n",
      "country 0.8901098901098901\n",
      "jazz 0\n",
      "disco 0\n",
      "reggae 0.9230769230769231\n",
      "metal 0\n",
      "classical 0\n",
      "blues 0\n",
      "MISSES\n",
      "pop 0.4915254237288136\n",
      "hiphop 0.430327868852459\n",
      "rock 0.8924731182795699\n",
      "country 0.6666666666666666\n",
      "jazz 0.8947368421052632\n",
      "disco 0.8\n",
      "reggae 0.7333333333333333\n",
      "metal 1.0\n",
      "classical 1.0\n",
      "blues 0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "print(\"COMBINED\")\n",
    "for genre, accuracy in combined_genre_accs.items():\n",
    "    print(genre, accuracy)\n",
    "\n",
    "print(\"HITS\")\n",
    "for genre, accuracy in bb_genre_accs.items():\n",
    "    print(genre, accuracy)\n",
    "\n",
    "print(\"MISSES\")\n",
    "for genre, accuracy in rand_genre_accs.items():\n",
    "    print(genre, accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
