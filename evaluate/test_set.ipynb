{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_path = \"C:/Users/arwin/Documents/dev/APS360-PROJECT/data\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigModel(nn.Module):\n",
    "  def  __init__(self):\n",
    "    super(BigModel, self).__init__()\n",
    "    # CNN\n",
    "    resnet18 = models.resnet18(pretrained=True)\n",
    "    self.features = nn.Sequential(*(list(resnet18.children())[:-2])) # I removed the last two layers of resnet and replaced them with the ones said in the paper\n",
    "    self.features.add_module(\"pool\", nn.AdaptiveMaxPool2d((1,1)))\n",
    "\n",
    "    self.fc1_cnn = nn.Linear(512, 256)\n",
    "    self.dropout_cnn = nn.Dropout(p = 0.5)\n",
    "    self.batch_norm_cnn = nn.BatchNorm1d(256)\n",
    "    \n",
    "    # RNN\n",
    "    self.bi_gru = nn.GRU(input_size=672, hidden_size = 256, num_layers =1, batch_first=True, bidirectional=True) # I dont know what the size of the input to the bi-gru is supposed to be\n",
    "    self.fc_gru = nn.Linear(512, 256)\n",
    "    \n",
    "    # FC LAYERS\n",
    "    self.dropout = nn.Dropout(p = 0.5)\n",
    "    # self.fc = nn.Linear(512, 1)\n",
    "    self.fc = nn.Linear(512, 10) # becuase 10 genre classes\n",
    "    # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # CNN\n",
    "    # print(f\"Shape before ResNet features: {x.shape}\")  # Debug statement 1\n",
    "    x1 = self.features(x)\n",
    "    # print(f\"Shape after ResNet features: {x1.shape}\")  # Debug statement 1\n",
    "    x1 = torch.flatten(x1, 1)\n",
    "    # print(f\"Shape after flattening: {x1.shape}\")  # Debug statement 2\n",
    "    x1 = self.fc1_cnn(x1)\n",
    "    x1 = self.dropout_cnn(x1)\n",
    "    x1 = self.batch_norm_cnn(x1)\n",
    "    \n",
    "    # Bi-GRU\n",
    "    batch_size, channels, height, width = x.shape\n",
    "    x2 = x.view(batch_size, height, -1)\n",
    "    # print(f\"Shape before Bi-GRU: {x2.shape}\")  # Debug statement 4\n",
    "    x2, _ = self.bi_gru(x2)\n",
    "    x2 = torch.cat((x2[:, -1, :256], x2[:, 0, 256:]), dim=1)\n",
    "    x2 = self.fc_gru(x2)\n",
    "\n",
    "    # Concat Outputs of each model\n",
    "    x = torch.cat((x1, x2), -1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc(x)\n",
    "    # x = self.sigmoid(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_name(batch_size, learning_rate, epoch, weight_decay):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"transfermodel_bs{0}_lr{1}_epoch{2}_wd{3}\".format(\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch,\n",
    "                                                   weight_decay)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_model_for_binary_classification(model):\n",
    "    # Freeze feature layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.bi_gru.parameters():\n",
    "        param.requires_grad = False\n",
    "    # for param in model.fc1_cnn.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    # for param in model.fc_gru.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    # for param in model.batch_norm_cnn.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # Remove and add new FC layers for binary classification\n",
    "    # Adjust the input features of the first linear layer according to your new architecture\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(512, 256),  # Adjust the input size if needed\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(256, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, img_dirs, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dirs (dict): Dictionary with keys 'hit' and 'miss' and their respective image directory paths.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.img_labels = []\n",
    "        self.img_files = []\n",
    "        for label, dir_path in img_dirs.items():\n",
    "            for file in os.listdir(dir_path):\n",
    "                if file.endswith('.png'):\n",
    "                    self.img_files.append(os.path.join(dir_path, file))\n",
    "                    self.img_labels.append(1 if label=='hit' else 0)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.img_labels[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, torch.tensor(label, dtype=torch.float) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((224, 224)),  \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "img_dirs = {\n",
    "    'hit': data_path + '/billboard_spec',\n",
    "    'miss': data_path + '/random_spec'\n",
    "}\n",
    "\n",
    "# Initialize your dataset\n",
    "dataset = CustomImageDataset(img_dirs=img_dirs, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size + test_size])\n",
    "val_dataset, test_dataset = torch.utils.data.random_split(val_test_dataset, [val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BigModel()\n",
    "path_to_saved_model = 'C:/Users/arwin/Documents/dev/APS360-PROJECT/genre_classifier/genre_ensemble_model_batch256_lr0.001_weightdecay3e-05.pth' \n",
    "model.load_state_dict(torch.load(path_to_saved_model))\n",
    "model = modify_model_for_binary_classification(model)\n",
    "model = model.to(device)\n",
    "\n",
    "batch_size = 128\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 75\n",
    "weight_decay = 0\n",
    "save = True\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_running_loss = 0.0\n",
    "test_running_corrects = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device).long()  # Adjust for label dimensions\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        acc = get_accuracy(outputs, labels)\n",
    "\n",
    "        test_running_loss += loss.item() * inputs.size(0)\n",
    "        test_running_corrects += acc.item() * inputs.size(0)\n",
    "\n",
    "test_loss = test_running_loss / len(test_loader.dataset)\n",
    "test_acc = test_running_corrects / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
