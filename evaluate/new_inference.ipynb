{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import skimage.io\n",
    "from skimage.transform import resize\n",
    "\n",
    "data_path = \"C:/Users/arwin/Documents/dev/APS360-PROJECT/data\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BigModel(nn.Module):\n",
    "  def  __init__(self):\n",
    "    super(BigModel, self).__init__()\n",
    "    # CNN\n",
    "    resnet18 = models.resnet18(pretrained=True)\n",
    "    self.features = nn.Sequential(*(list(resnet18.children())[:-2])) # I removed the last two layers of resnet and replaced them with the ones said in the paper\n",
    "    self.features.add_module(\"pool\", nn.AdaptiveMaxPool2d((1,1)))\n",
    "\n",
    "    self.fc1_cnn = nn.Linear(512, 256)\n",
    "    self.dropout_cnn = nn.Dropout(p = 0.5)\n",
    "    self.batch_norm_cnn = nn.BatchNorm1d(256)\n",
    "    \n",
    "    # RNN\n",
    "    self.bi_gru = nn.GRU(input_size=672, hidden_size = 256, num_layers =1, batch_first=True, bidirectional=True) # I dont know what the size of the input to the bi-gru is supposed to be\n",
    "    self.fc_gru = nn.Linear(512, 256)\n",
    "    \n",
    "    # FC LAYERS\n",
    "    self.dropout = nn.Dropout(p = 0.5)\n",
    "    # self.fc = nn.Linear(512, 1)\n",
    "    self.fc = nn.Linear(512, 10) # becuase 10 genre classes\n",
    "    # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "  def forward(self, x):\n",
    "    # CNN\n",
    "    # print(f\"Shape before ResNet features: {x.shape}\")  # Debug statement 1\n",
    "    x1 = self.features(x)\n",
    "    # print(f\"Shape after ResNet features: {x1.shape}\")  # Debug statement 1\n",
    "    x1 = torch.flatten(x1, 1)\n",
    "    # print(f\"Shape after flattening: {x1.shape}\")  # Debug statement 2\n",
    "    x1 = self.fc1_cnn(x1)\n",
    "    x1 = self.dropout_cnn(x1)\n",
    "    x1 = self.batch_norm_cnn(x1)\n",
    "    \n",
    "    # Bi-GRU\n",
    "    batch_size, channels, height, width = x.shape\n",
    "    x2 = x.view(batch_size, height, -1)\n",
    "    # print(f\"Shape before Bi-GRU: {x2.shape}\")  # Debug statement 4\n",
    "    x2, _ = self.bi_gru(x2)\n",
    "    x2 = torch.cat((x2[:, -1, :256], x2[:, 0, 256:]), dim=1)\n",
    "    x2 = self.fc_gru(x2)\n",
    "\n",
    "    # Concat Outputs of each model\n",
    "    x = torch.cat((x1, x2), -1)\n",
    "    x = self.dropout(x)\n",
    "    x = self.fc(x)\n",
    "    # x = self.sigmoid(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_model_name(batch_size, learning_rate, epoch, weight_decay):\n",
    "    \"\"\" Generate a name for the model consisting of all the hyperparameter values\n",
    "\n",
    "    Args:\n",
    "        config: Configuration object containing the hyperparameters\n",
    "    Returns:\n",
    "        path: A string with the hyperparameter name and value concatenated\n",
    "    \"\"\"\n",
    "    path = \"transfermodel_bs{0}_lr{1}_epoch{2}_wd{3}\".format(\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch,\n",
    "                                                   weight_decay)\n",
    "    return path\n",
    "\n",
    "def modify_model_for_binary_classification(model):\n",
    "    # Freeze feature layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.bi_gru.parameters():\n",
    "        param.requires_grad = False\n",
    "    # for param in model.fc1_cnn.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    # for param in model.fc_gru.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    # for param in model.batch_norm_cnn.parameters():\n",
    "    #     param.requires_grad = False\n",
    "    \n",
    "    # Remove and add new FC layers for binary classification\n",
    "    # Adjust the input features of the first linear layer according to your new architecture\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Linear(512, 256),  # Adjust the input size if needed\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(p=0.5),\n",
    "        nn.Linear(256, 1),\n",
    "        nn.Sigmoid()\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def audio_to_spec(audio_path, out_fname=\"temp.png\", sz = (224, 3*224)):\n",
    "    \n",
    "    # 1. CONVERT AUDIO FILE TO SPEC\n",
    "    song_data, song_sr = librosa.load(os.path.join(dir, audio_path), sr=None)\n",
    "    # song_data = librosa.feature.melspectrogram(y=song_data, sr=song_sr, n_mels=sz[1], n_fft=1024, hop_length=512, fmax=8000)\n",
    "    song_data = librosa.feature.melspectrogram(y=song_data, sr=song_sr, n_mels=256, n_fft=2048, hop_length=512*25)\n",
    "\n",
    "    # 2. Resize for ResNet18 input dims, with height = 224, width = 224*3\n",
    "    song_data = resize(song_data, sz, order=0, mode='reflect', anti_aliasing=False)\n",
    "\n",
    "    # 3. Split image into 3 slices by width and insert into each R G B channel\n",
    "    assert(song_data.shape[1] // 3 == 224)\n",
    "    channel_sz = song_data.shape[1] // 3\n",
    "    r, g, b = song_data[:, :channel_sz], song_data[:, channel_sz : 2 * channel_sz], song_data[:, 2 * channel_sz : 3 * channel_sz]\n",
    "    \n",
    "    # 4. a - LOG, b - NORM, c - REFLECT, d - INVERT\n",
    "    def norm_array(X, min=0.0, max=1.0):\n",
    "        X_std = (X - X.min()) / (X.max() - X.min())\n",
    "        X_scaled = X_std * (max - min) + min\n",
    "        return X_scaled\n",
    "    \n",
    "    def prepare_array(arr):\n",
    "        arr = np.log(arr + 1e-9)\n",
    "        arr = norm_array(arr,0, 255).astype(np.uint8)\n",
    "        arr = np.flip(arr, axis=0)\n",
    "        arr = 255 - arr\n",
    "        return arr\n",
    "\n",
    "    r = prepare_array(r)\n",
    "    g = prepare_array(g)\n",
    "    b = prepare_array(b)\n",
    "    img = np.stack([r, g, b], axis=-1)\n",
    "\n",
    "\n",
    "    # 5. Save to img\n",
    "    skimage.io.imsave(out_fname + \".png\", img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
      "             ReLU-14           [-1, 64, 56, 56]               0\n",
      "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
      "             ReLU-17           [-1, 64, 56, 56]               0\n",
      "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
      "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
      "             ReLU-21          [-1, 128, 28, 28]               0\n",
      "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
      "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
      "             ReLU-26          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
      "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
      "             ReLU-30          [-1, 128, 28, 28]               0\n",
      "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
      "             ReLU-33          [-1, 128, 28, 28]               0\n",
      "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
      "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "             ReLU-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
      "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
      "             ReLU-42          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
      "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
      "             ReLU-46          [-1, 256, 14, 14]               0\n",
      "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
      "             ReLU-49          [-1, 256, 14, 14]               0\n",
      "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
      "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-53            [-1, 512, 7, 7]               0\n",
      "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-58            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
      "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-62            [-1, 512, 7, 7]               0\n",
      "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
      "             ReLU-65            [-1, 512, 7, 7]               0\n",
      "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
      "AdaptiveMaxPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 256]         131,328\n",
      "          Dropout-69                  [-1, 256]               0\n",
      "      BatchNorm1d-70                  [-1, 256]             512\n",
      "              GRU-71  [[-1, 224, 512], [-1, 2, 256]]               0\n",
      "           Linear-72                  [-1, 256]         131,328\n",
      "          Dropout-73                  [-1, 512]               0\n",
      "           Linear-74                  [-1, 256]         131,328\n",
      "             ReLU-75                  [-1, 256]               0\n",
      "          Dropout-76                  [-1, 256]               0\n",
      "           Linear-77                    [-1, 1]             257\n",
      "          Sigmoid-78                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 11,571,265\n",
      "Trainable params: 394,753\n",
      "Non-trainable params: 11,176,512\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 385.20\n",
      "Params size (MB): 44.14\n",
      "Estimated Total Size (MB): 429.91\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_model = os.path.normpath(\"C:/Users/arwin/Documents/dev/APS360-PROJECT/transfer_learning/transfermodel_bs512_lr0.001_epoch100_wd0/transfermodel_bs512_lr0.001_epoch100_wd0.pth\")\n",
    "\n",
    "model = BigModel()\n",
    "model = modify_model_for_binary_classification(model)\n",
    "model.load_state_dict(torch.load(best_model))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "summary(model, input_size=(3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arwin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\arwin\\anaconda3\\envs\\torch\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is not popular 6.031413249729667e-06\n"
     ]
    }
   ],
   "source": [
    "model = BigModel()\n",
    "model = modify_model_for_binary_classification(model)\n",
    "model.load_state_dict(torch.load(best_model))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    random_1 = 'C:/Users/arwin/Documents/dev/APS360-PROJECT/data/random_spec/ \\'98_Erreur Irrcuprable ().png'\n",
    "    random_2 = 'C:/Users/arwin/Documents/dev/APS360-PROJECT/data/random_spec/Altus_The Spontaneous Overflow of Powerful Feelings.png'\n",
    "    hit = 'C:/Users/arwin/Documents/dev/APS360-PROJECT/data/billboard_spec/5 Seconds Of Summer_Girls Talk Boys.png'\n",
    "    # image = transforms.ToTensor()(Image.open('inference_specs/dj perfekt_somani money.png').convert('RGB'))\n",
    "    image = transforms.ToTensor()(Image.open(random_2).convert('RGB'))\n",
    "\n",
    "    image = image.to(device)\n",
    "    output = model(image.unsqueeze(0))  # batch dimension\n",
    "    \n",
    "    if output.item() >= 0.5:\n",
    "        print(\"is billboard hit!\", output.item())\n",
    "    else:\n",
    "        print(\"is not popular\", output.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
